{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "ded7a22b",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "ded7a22b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "4a6b6bf0",
      "metadata": {
        "id": "4a6b6bf0"
      },
      "outputs": [],
      "source": [
        "# ─────────────────────────────────────────────\n",
        "# Parameters\n",
        "# ─────────────────────────────────────────────\n",
        "N             = 3       # Number of agents (keep odd)\n",
        "ROUNDS        = 50000     # Total rounds to simulate\n",
        "MEMORY        = 5        # History length m (state space = 2^m)\n",
        "ALPHA         = 0.1      # Learning rate\n",
        "GAMMA         = 0.9      # Discount factor\n",
        "EPSILON       = 0.2      # Initial exploration rate\n",
        "EPSILON_DECAY = 0.997    # Multiplicative decay per round\n",
        "EPSILON_MIN   = 0.01     # Floor for exploration\n",
        "WINDOW        = 50       # Rolling average window\n",
        "SEED          = 42       # For reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "\n",
        "# ----------------------------\n",
        "# Minority Game Environment (N=3)\n",
        "# ----------------------------\n",
        "class MinorityGame:\n",
        "    \"\"\"\n",
        "    N=3, actions in {0,1}.\n",
        "    Minority side (strictly fewer agents) gets reward 1, others 0.\n",
        "    Public history signal: minority_action (0/1) if exists else 0.\n",
        "    History length = m bits -> integer state in [0, 2^m - 1].\n",
        "    \"\"\"\n",
        "    def __init__(self, m=5, seed=0):\n",
        "        self.n_agents = 3\n",
        "        self.m = m\n",
        "        self.base_states = 2 ** m\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.t = 0\n",
        "        self.hist = deque([0] * self.m, maxlen=self.m)\n",
        "        return self._hist_to_int()\n",
        "\n",
        "    def _hist_to_int(self):\n",
        "        s = 0\n",
        "        for b in self.hist:\n",
        "            s = (s << 1) | int(b)\n",
        "        return s\n",
        "\n",
        "    def step(self, actions):\n",
        "        actions = np.asarray(actions, dtype=int)\n",
        "        ssum = int(actions.sum())\n",
        "\n",
        "        rewards = np.zeros(3, dtype=float)\n",
        "        minority_action = None\n",
        "\n",
        "        # N=3: minority exists iff sum==1 (minority=1) or sum==2 (minority=0)\n",
        "        if ssum == 1:\n",
        "            minority_action = 1\n",
        "        elif ssum == 2:\n",
        "            minority_action = 0\n",
        "\n",
        "        if minority_action is not None:\n",
        "            winners = (actions == minority_action)\n",
        "            rewards[winners] = 1.0\n",
        "\n",
        "        # public signal for next state (0 if no minority)\n",
        "        signal = 0 if minority_action is None else int(minority_action)\n",
        "        self.hist.append(signal)\n",
        "\n",
        "        self.t += 1\n",
        "        next_state = self._hist_to_int()\n",
        "        return next_state, rewards, minority_action\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Basic Q-learning Agent\n",
        "# ----------------------------\n",
        "class QAgent:\n",
        "    def __init__(self, n_states, n_actions=2, alpha=0.1, gamma=0.95,\n",
        "                 eps_start=1.0, eps_end=0.05, eps_decay=0.999, seed=0):\n",
        "        self.Q = np.zeros((n_states, n_actions), dtype=float)\n",
        "        self.alpha = float(alpha)\n",
        "        self.gamma = float(gamma)\n",
        "        self.eps = float(eps_start)\n",
        "        self.eps_start = float(eps_start)\n",
        "        self.eps_end = float(eps_end)\n",
        "        self.eps_decay = float(eps_decay)\n",
        "        self.n_actions = int(n_actions)\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "\n",
        "    def act(self, s):\n",
        "        if self.rng.random() < self.eps:\n",
        "            return int(self.rng.integers(self.n_actions))\n",
        "        q = self.Q[s]\n",
        "        max_q = np.max(q)\n",
        "        max_actions = np.flatnonzero(np.isclose(q, max_q))\n",
        "        return int(self.rng.choice(max_actions))\n",
        "\n",
        "    def update(self, s, a, r, s_next):\n",
        "        td_target = r + self.gamma * np.max(self.Q[s_next])\n",
        "        self.Q[s, a] += self.alpha * (td_target - self.Q[s, a])\n",
        "\n",
        "    def decay_eps(self):\n",
        "        self.eps = max(self.eps_end, self.eps * self.eps_decay)\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Train: independent Q-learning (all update every step)\n",
        "# ----------------------------\n",
        "def run_basic_qlearning(\n",
        "    m=3,\n",
        "    steps=100000,\n",
        "    seed=0,\n",
        "    alpha=0.12,\n",
        "    gamma=0.95,\n",
        "    eps_start=0.8,\n",
        "    eps_end=0.02,\n",
        "    eps_decay=0.9997,\n",
        "    log_window=2000\n",
        "):\n",
        "    env = MinorityGame(m=m, seed=seed)\n",
        "    n_states = env.base_states\n",
        "\n",
        "    agents = [\n",
        "        QAgent(n_states, alpha=alpha, gamma=gamma,\n",
        "               eps_start=eps_start, eps_end=eps_end, eps_decay=eps_decay,\n",
        "               seed=seed + 10 + i)\n",
        "        for i in range(3)\n",
        "    ]\n",
        "\n",
        "    s = env.reset()\n",
        "    rewards_hist = np.zeros((steps, 3), dtype=float)\n",
        "\n",
        "    for t in range(steps):\n",
        "        actions = [ag.act(s) for ag in agents]\n",
        "        s_next, rewards, _ = env.step(actions)\n",
        "\n",
        "        # all agents update every step (independent learners)\n",
        "        for i, ag in enumerate(agents):\n",
        "            ag.update(s, actions[i], rewards[i], s_next)\n",
        "            ag.decay_eps()\n",
        "\n",
        "        rewards_hist[t] = rewards\n",
        "        s = s_next\n",
        "\n",
        "    # rolling win-rate\n",
        "    win_rate = np.zeros((steps, 3), dtype=float)\n",
        "    for i in range(3):\n",
        "        csum = np.cumsum(rewards_hist[:, i])\n",
        "        for t in range(steps):\n",
        "            lo = max(0, t - log_window + 1)\n",
        "            total = csum[t] - (csum[lo - 1] if lo > 0 else 0.0)\n",
        "            win_rate[t, i] = total / (t - lo + 1)\n",
        "\n",
        "    return rewards_hist, win_rate\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    rewards_hist, win_rate = run_basic_qlearning(\n",
        "        m=5,\n",
        "        steps=50000,\n",
        "        seed=18,\n",
        "        alpha=0.12,\n",
        "        gamma=0.95,\n",
        "        eps_start=0.8,\n",
        "        eps_end=0.02,\n",
        "        eps_decay=0.9997,\n",
        "        log_window=2000\n",
        "    )\n",
        "\n",
        "    t = np.arange(rewards_hist.shape[0])\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(t, win_rate[:, 0], label=\"agent 0 rolling win-rate\")\n",
        "    plt.plot(t, win_rate[:, 1], label=\"agent 1 rolling win-rate\")\n",
        "    plt.plot(t, win_rate[:, 2], label=\"agent 2 rolling win-rate\")\n",
        "    plt.axhline(1/3, linestyle=\"--\", linewidth=1, label=\"target 1/3\")\n",
        "    plt.xlabel(\"time\")\n",
        "    plt.ylabel(\"win-rate\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Basic Independent Q-learning on Minority Game\")\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Final avg win-rate (last 5000 steps):\", rewards_hist[-5000:].mean(axis=0))\n",
        "    print(\"Total avg win-rate:\", rewards_hist.mean(axis=0))"
      ],
      "metadata": {
        "id": "YGuDUYdQluTe"
      },
      "id": "YGuDUYdQluTe",
      "execution_count": 26,
      "outputs": []
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}